{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "from  nltk import pos_tag\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import langdetect\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# code starts here!\n",
    "stop_words_en = set(stopwords.words(\"english\"))\n",
    "stemmer_en = SnowballStemmer('english')\n",
    "\n",
    "SOLUTION_FOR_EVERYTHING = 42\n",
    "\n",
    "# %env CUDA_VISIBLE_DEVICES='0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labelled_dataset = pd.read_pickle(\"data/labelled_dataset.pickle\")\n",
    "\n",
    "labelled_dataset = labelled_dataset.truncate(after=200)\n",
    "\n",
    "labelled_dataset.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_dataset.labelmax.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_dataset = labelled_dataset[labelled_dataset.labelmax != \"null\"]\n",
    "labelled_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_array = sorted(list(labelled_dataset.labelmax.unique()))\n",
    "classes_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_dataset = labelled_dataset.drop_duplicates()\n",
    "labelled_dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing non-English texts\n",
    "By chance, I found that are also texts in German, French, Spanish, and perhaps in some other languages. For the sake of performance, it will only be trained (and accept) English texts. Further improvements can change this.\n",
    "\n",
    "There are some Python libraries that can help us on the task of langauge classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_language(text):\n",
    "    return langdetect.detect(text)\n",
    "    \n",
    "labelled_dataset = labelled_dataset[labelled_dataset[\"text\"].apply(text_language) == \"en\"]\n",
    "labelled_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_dataset.labelmax.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation + Tokenization + Stemming + Stop-word removal\n",
    "\n",
    "Here was concentrated most of the work of this challenge: preparing the text in order to be processed.\n",
    "\n",
    "Just an additional **trick** added here. Appart from the standard NLP pre-propressing tasks, as the segments of a review text feature the positive and negative aspects of the review, I hypothesize that textual expressions from each of these segments (pros/cons) may have a different influence to the type company culture determination.\n",
    "\n",
    "For that reason, appart from segmentate a text into pros/cons parts, I'm also adding the text with a prefix (+/-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titles of the sehments\n",
    "SEGMENT_TITLES = '(Pros |Cons |Advice to Management )'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a segment by it's index in a string splited by SEGMENT_TITLES (i.e.: pros= 2, cons = 4, ...)\n",
    "def filter_by_segment(entry, index):\n",
    "    segments = re.split(SEGMENT_TITLES, entry)\n",
    "    return segments[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs tokenisation + stemming + stop-word removal\n",
    "def tok_stem_stop(segment):\n",
    "    # tokenize and remove empty entries\n",
    "    tokens = nltk.word_tokenize(segment.lower())\n",
    "    # stop-word removal, single character tokens removal, and stemming\n",
    "    tokens = [stemmer_en.stem(_token) for _token in tokens\n",
    "              if _token not in stop_words_en\n",
    "              and len(_token) > 1\n",
    "              and _token not in [\"'s\", \"'d\"]\n",
    "             ]\n",
    "    # n't -> not\n",
    "    tokens = [_token.replace(\"n't\", \"not\") for _token in tokens]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs tokenisation + gather just words with POS NN/JJ/VB* + stemming\n",
    "def tok_pos_stem(segment):\n",
    "    # tokenize and remove empty entries\n",
    "    tokens = nltk.word_tokenize(segment.lower())\n",
    "    # choosing only nouns, verbs, and adjectives\n",
    "    tokens = [_pos[0] for _pos in pos_tag(tokens) if _pos[1] in [\"NN\", \"NNS\", \"JJ\", \"VB\", \"VBG\"]]\n",
    "    # stemming\n",
    "    tokens = [stemmer_en.stem(_token) for _token in tokens]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs tokenisation + lower case\n",
    "def tokens(segment):\n",
    "    return nltk.word_tokenize(segment.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs tokenisation + stop-word removal + lower case\n",
    "def token_stop(segment):\n",
    "    tokens = nltk.word_tokenize(segment.lower())\n",
    "    tokens = [_token for _token in tokens\n",
    "              if _token not in stop_words_en\n",
    "              and len(_token) > 1\n",
    "              and _token not in [\"'s\", \"'d\"]\n",
    "             ]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds a prefix (e.g.: +/-) to a token\n",
    "def prefix(entry, _prefix):\n",
    "    return \" \".join([_prefix + _token for _token in entry.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding columns with pre-processed data to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_dataset[\"pros\"] = labelled_dataset[\"text\"].apply(lambda entry: filter_by_segment(entry, 2));\n",
    "labelled_dataset[\"cons\"] = labelled_dataset[\"text\"].apply(lambda entry: filter_by_segment(entry, 4));\n",
    "labelled_dataset[\"all\"] = labelled_dataset[\"pros\"] + \" \" + labelled_dataset[\"cons\"];\n",
    "# labelled_dataset[\"advice\"] = labelled_dataset[\"text\"].apply(lambda entry: filter_by_segment(entry, 6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_dataset[\"pros_tokens\"] = labelled_dataset[\"pros\"].apply(token_stop);\n",
    "labelled_dataset[\"cons_tokens\"] = labelled_dataset[\"cons\"].apply(token_stop);\n",
    "labelled_dataset[\"all_tokens\"] = labelled_dataset[\"all\"].apply(token_stop);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_dataset[\"pros_tok_stem_stop\"] = labelled_dataset[\"pros\"].apply(tok_stem_stop);\n",
    "labelled_dataset[\"pros_tok_stem_stop\"] = labelled_dataset[\"pros_tok_stem_stop\"].apply(lambda entry: prefix(entry, \"+\"));\n",
    "labelled_dataset[\"cons_tok_stem_stop\"] = labelled_dataset[\"cons\"].apply(tok_stem_stop);\n",
    "labelled_dataset[\"cons_tok_stem_stop\"] = labelled_dataset[\"cons_tok_stem_stop\"].apply(lambda entry: prefix(entry, \"-\"));\n",
    "\n",
    "labelled_dataset[\"all_tok_stem_stop\"] = labelled_dataset[\"pros_tok_stem_stop\"] + \" \" + \\\n",
    "    labelled_dataset[\"cons_tok_stem_stop\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_dataset[\"pros_tok_pos_stem\"] = labelled_dataset[\"pros\"].apply(tok_pos_stem);\n",
    "labelled_dataset[\"pros_tok_pos_stem\"] = labelled_dataset[\"pros_tok_pos_stem\"].apply(lambda entry: prefix(entry, \"+\"));\n",
    "labelled_dataset[\"cons_tok_pos_stem\"] = labelled_dataset[\"cons\"].apply(tok_pos_stem);\n",
    "labelled_dataset[\"cons_tok_pos_stem\"] = labelled_dataset[\"cons_tok_pos_stem\"].apply(lambda entry: prefix(entry, \"-\"));\n",
    "\n",
    "labelled_dataset[\"all_tok_pos_stem\"] = labelled_dataset[\"pros_tok_pos_stem\"] + \" \" + \\\n",
    "    labelled_dataset[\"cons_tok_pos_stem\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews length\n",
    "\n",
    "Tha mnajority of the reviews contain less than 140 words.\n",
    "**Pros** use to be longer than **cons** (people uses to complain less on Glassdoor?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_lens = labelled_dataset[\"all_tokens\"].apply(len)\n",
    "plt.xlim(0, 200)\n",
    "sns.boxplot(str_lens, palette=\"Set3\", showfliers=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_lens = labelled_dataset[\"pros_tokens\"].apply(len)\n",
    "plt.xlim(0, 200)\n",
    "sns.boxplot(str_lens, palette=\"Set2\", showfliers=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_lens = labelled_dataset[\"cons_tokens\"].apply(len)\n",
    "plt.xlim(0, 200)\n",
    "sns.boxplot(str_lens, palette=\"Set1\", showfliers=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test datasets\n",
    "\n",
    "In this exploratory study, I'm going to split the dataset into 80% training and 20% testing. It must be ensured that the same class proportions are kept in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\"text\", \"labelmax\", \"pros\", \"cons\", \"all\", \"pros_tokens\", \"cons_tokens\", \"all_tokens\", \"pros_tok_stem_stop\", \"cons_tok_stem_stop\", \"all_tok_stem_stop\", \"pros_tok_pos_stem\", \"cons_tok_pos_stem\", \"all_tok_pos_stem\"]\n",
    "\n",
    "df_train, df_test, _, _ = train_test_split(\n",
    "    labelled_dataset[COLUMNS],\n",
    "    labelled_dataset[[\"labelmax\"]],\n",
    "    stratify=labelled_dataset[[\"labelmax\"]],\n",
    "    test_size=0.20,\n",
    "    random_state=SOLUTION_FOR_EVERYTHING\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing classes\n",
    "\n",
    "Classes are not balanced and it may result in a skewed classification based in this data. One solution for this is to oversample minorty classes. As **detail** and **integrity** classes have small subsets, oversampling can also result in skewed results. I'm taking the risk and oversampling all of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df_train, x=\"labelmax\", palette=\"Set3\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _label in set(df_train.labelmax.values):\n",
    "    ballanced_label_df = resample(\n",
    "        df_train[df_train.labelmax == _label],\n",
    "        replace=True,\n",
    "        n_samples=max(df_train.labelmax.value_counts()),\n",
    "        random_state=42\n",
    "    )\n",
    "    df_train = df_train[df_train.labelmax != _label]\n",
    "    df_train = pd.concat([df_train, ballanced_label_df])\n",
    "\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df_train, x=\"labelmax\", palette=\"Set3\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical TF-IDF approach\n",
    "\n",
    "The classical approach for text classification uses TF-IDF. In this case, I'm using a SVM instead of a Naïve-Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf():\n",
    "    count_vectorizer = CountVectorizer(ngram_range=(1, 1), max_features=500)\n",
    "    tf_idf = TfidfTransformer(norm=\"l2\")\n",
    "    svm_clf = SVC()\n",
    "    tfidf_clf = Pipeline([\n",
    "        ('count_vectorizer', count_vectorizer),\n",
    "        ('tf_idf', tf_idf),\n",
    "        ('svm_clf', svm_clf)\n",
    "    ])\n",
    "    return tfidf_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df_train.as_matrix(columns=[\"all_tok_pos_stem\"])[:,0]\n",
    "train_labels = df_train.as_matrix(columns=[\"labelmax\"])[:,0]\n",
    "text_clf.fit(train_data, train_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_clf.predict(df_test.as_matrix(columns=[\"all_tok_pos_stem\"])[:,0])\n",
    "labels = list(df_test.labelmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(labels, predictions)\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap=\"BuPu\",\n",
    "            xticklabels=df_test.labelmax.unique(), yticklabels=df_test.labelmax.unique());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels, predictions, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN (3+3+2) approach\n",
    "\n",
    "CNNs are among the state-of-the-art of text classification. This is the most well-known CNN implementation (Yoon Kim 2014) for this type of task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Embedding, Dense, Convolution1D, ThresholdedReLU, MaxPooling1D, Dropout, Flatten\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LambdaCallback\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word vectors from Glove, imported as gensim keyvecs\n",
    "wv = Word2VecKeyedVectors.load(\"models/wv/word.vectors\")\n",
    "weights = np.load(\"models/wv/word.vectors.vectors.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_DIM = 200 # fulfills most of the reviews with 140 words\n",
    "CLASSES_DIM = len(classes_array) # 6\n",
    "DROPOUT = 0.1 # maybe..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn labels into categorical arrays\n",
    "train_labelmax = [classes_array.index(row) for row in df_train[\"labelmax\"].as_matrix()]\n",
    "test_labelmax = [classes_array.index(row) for row in df_test[\"labelmax\"].as_matrix()]\n",
    "\n",
    "train_cat_labels = to_categorical(np.asarray(train_labelmax))\n",
    "test_cat_labels = to_categorical(np.asarray(test_labelmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_vectors(df):\n",
    "    \n",
    "    def vectorise_token(token, wv):\n",
    "        # just ignoring OOVs\n",
    "        vector = None\n",
    "        if token in wv.vocab:\n",
    "            vector = wv.index2entity.index(token)\n",
    "        return vector\n",
    "        \n",
    "    embedding_vectors = []\n",
    "    \n",
    "    for review in df:\n",
    "        review_embedding = []\n",
    "        for token in review:\n",
    "            vector = vectorise_token(token, wv)\n",
    "            if vector is not None:\n",
    "                review_embedding.append(vector)\n",
    "        embedding_vectors.append(review_embedding)\n",
    "\n",
    "    return embedding_vectors\n",
    "\n",
    "train_embedding = create_embedding_vectors(df_train[\"all_tokens\"])\n",
    "test_embedding = create_embedding_vectors(df_test[\"pros_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = pad_sequences(train_embedding, maxlen=SEQUENCE_DIM, padding='post')\n",
    "test_vectors = pad_sequences(test_embedding, maxlen=SEQUENCE_DIM, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the convnet here!\n",
    "def convnet(weights, sequence_dim, classes_dim, dropout_val):\n",
    "    _lexicon_dim = weights.shape[0]\n",
    "    _embed_dim = weights.shape[1]\n",
    "    inputs = Input(shape=(sequence_dim,), dtype='int32')\n",
    "    embed = Embedding(input_dim=_lexicon_dim, output_dim=_embed_dim, weights=[weights])(inputs)\n",
    "\n",
    "    x = Convolution1D(100, 5, activation='relu')(embed)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Convolution1D(100, 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(4)(x)\n",
    "    x = Convolution1D(100, 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(3)(x)\n",
    "\n",
    "    flatten = Flatten()(x)\n",
    "    dropout = Dropout(dropout_val)(flatten)\n",
    "    x = Dense(100, activation='relu')(dropout)\n",
    "\n",
    "    outputs = Dense(classes_dim, activation='softmax', name=\"last_layer\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = convnet(weights, SEQUENCE_DIM, CLASSES_DIM, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=1e-4, epsilon=1e-08), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make it faster and avoid the over-fitting that perhaps was arlready happening\n",
    "early_stop_cb = EarlyStopping(patience=7, monitor='val_acc', mode='max')\n",
    "checkpoint_cb = ModelCheckpoint(\"cnn.h5\", monitor='val_acc', save_best_only=True, mode='max', verbose=0)\n",
    "\n",
    "callbacks = [early_stop_cb, checkpoint_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_vectors, train_cat_labels, callbacks=callbacks, epochs=100, validation_split=0.10, shuffle=True, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history[\"val_acc\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h = {\"val_acc\": history.history[\"val_acc\"], \"epoch\": history.epoch}\n",
    "\n",
    "sns.pointplot(x=\"epoch\", y=\"val_acc\", data=pd.DataFrame.from_dict(h));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test loss and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_vectors, test_cat_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of 3 models is evaluated and compared.\n",
    "\n",
    "In the setup of this experiment, a SVM with stemming and stop-word removal performs better than the other models.\n",
    "\n",
    "It is statistical significant that CNN performs worse than SVMs in these setup conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MODELS = 20\n",
    "P = 0.05\n",
    "results_array = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating TF-IDF w/ tok+pos+stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for tfidf+tok+pos+stem\n",
    "train_data = df_train.as_matrix(columns=[\"all_tok_pos_stem\"])[:,0]\n",
    "train_labels = df_train.as_matrix(columns=[\"labelmax\"])[:,0]\n",
    "test_data = df_test.as_matrix(columns=[\"all_tok_pos_stem\"])[:,0]\n",
    "test_labels = list(df_test.labelmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(0, 1):\n",
    "    clf = tfidf()\n",
    "    clf.fit(train_data, train_labels);\n",
    "    predictions = clf.predict(test_data)\n",
    "    accuracy = accuracy_score(test_labels, predictions, normalize=True)\n",
    "    result = {\"model\": \"tfidf+tok+pos+stem\", \"acc\": accuracy}\n",
    "    results_array.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating TF-IDF w/ tok+stem+stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for tfidf+tok+stem+stop\n",
    "train_data = df_train.as_matrix(columns=[\"all_tok_stem_stop\"])[:,0]\n",
    "train_labels = df_train.as_matrix(columns=[\"labelmax\"])[:,0]\n",
    "test_data = df_test.as_matrix(columns=[\"all_tok_stem_stop\"])[:,0]\n",
    "test_labels = list(df_test.labelmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(0, 1):\n",
    "    clf = tfidf()\n",
    "    clf.fit(train_data, train_labels);\n",
    "    predictions = clf.predict(test_data)\n",
    "    accuracy = accuracy_score(test_labels, predictions, normalize=True)\n",
    "    result = {\"model\": \"tfidf+tok+stem+stop\", \"acc\": accuracy}\n",
    "    results_array.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(0, 10):\n",
    "    model = convnet(weights, SEQUENCE_DIM, CLASSES_DIM, DROPOUT)\n",
    "    model.compile(optimizer=Adam(lr=1e-4, epsilon=1e-08), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(train_vectors, train_cat_labels, epochs=1, validation_split=0.10, shuffle=True, batch_size=50, verbose=0)\n",
    "    \n",
    "    accuracy = model.evaluate(test_vectors, test_cat_labels)[1];\n",
    "    result = {\"model\": \"cnn\", \"acc\": accuracy}\n",
    "    results_array.append(result)\n",
    "    del model\n",
    "    gc.collect()\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"model\", y=\"acc\", data=pd.DataFrame.from_dict(results_array), palette=\"PRGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[result[\"acc\"] for result in results_array if result[\"model\"] == \"tfidf+tok+pos+stem\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[result[\"acc\"] for result in results_array if result[\"model\"] == \"tfidf+tok+stem+stop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr = [result[\"acc\"] for result in results_array if result[\"model\"] == \"cnn\"]\n",
    "st.t.interval(0.95, len(distr) - 1, loc=np.mean(distr), scale=st.sem(distr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
